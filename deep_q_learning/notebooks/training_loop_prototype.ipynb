{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 1: Tesla K40c (CNMeM is disabled, cuDNN 5004)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.nonlinearities import *\n",
    "import gym\n",
    "import numpy as np\n",
    "from skimage.color import rgb2gray\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def q_net(env):\n",
    "    height, width, nchannels = env.observation_space.shape\n",
    "    nchannels = 4 # we convert to black and white and use 4 prev frames\n",
    "    l_in = InputLayer((None, nchannels, height, width))\n",
    "    l_conv = Conv2DLayer(l_in, num_filters=32, filter_size=3, stride=2)\n",
    "    l_conv2 = Conv2DLayer(l_conv, num_filters=64, filter_size=3, stride=2)\n",
    "    l_conv3 = Conv2DLayer(l_conv2, num_filters=96, filter_size=3, stride=2)\n",
    "    l_conv4 = Conv2DLayer(l_conv3, num_filters=128, filter_size=3, stride=2)\n",
    "    l_dense = DenseLayer(l_conv4, num_units=env.action_space.n)\n",
    "    return l_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('X')\n",
    "y = T.fmatrix('y') # this is a row column\n",
    "action_mask = T.fmatrix('action_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 10:01:06,037] Making new env: Pong-v0\n",
      "[2017-03-19 10:01:06,104] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268742\n",
      "<lasagne.layers.input.InputLayer object at 0x7f4545429b90> (None, 4, 210, 160)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x7f4545429d90> (None, 32, 104, 79)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x7f4544ee1310> (None, 64, 51, 39)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x7f4544ee1690> (None, 96, 25, 19)\n",
      "<lasagne.layers.conv.Conv2DLayer object at 0x7f4544ee1a10> (None, 128, 12, 9)\n",
      "<lasagne.layers.dense.DenseLayer object at 0x7f4544ee1d90> (None, 6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "l_out = q_net(gym.make('Pong-v0'))\n",
    "print count_params(l_out)\n",
    "for layer in get_all_layers(l_out):\n",
    "    print layer, layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.imshow(env.step(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net_out = get_output(l_out, X)\n",
    "loss = ( y - (action_mask*net_out).sum(axis=1, keepdims=True) )**2\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_fn = theano.function([X], net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b, W, b]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_all_params(l_out, trainable=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "updates = lasagne.updates.rmsprop(loss, params, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([X,y,action_mask], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eps_greedy_action(phi_t, eps=0.1):\n",
    "    \"\"\"\n",
    "    phi_t: the pre-processed image for this time step\n",
    "    \"\"\"\n",
    "    if np.random.random() <= eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_dist = out_fn(phi_t[np.newaxis])\n",
    "        best_action = np.argmax(action_dist, axis=1)[0]\n",
    "        return best_action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:17:55,395] Making new env: Pong-v0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-62ef319ccb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m             phi_t1_minibatch = np.asarray(\n\u001b[1;32m     63\u001b[0m                 [ rand_transitions[i][\"phi_t1\"] for i in range(len(rand_transitions)) ]).astype(\"float32\")\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mqvalues_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_t1_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mmax_qvalues_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues_minibatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0my_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_fn' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "# set up replay buffer and image buffers\n",
    "buf_maxlen = 4\n",
    "buf = []\n",
    "buf_idx = []\n",
    "experience = []\n",
    "gamma = 1\n",
    "mb_size = 4\n",
    "eps = 1.0\n",
    "for t in range(10000):\n",
    "    # if the buffer is not at max length, do random actions\n",
    "    # to fill it up\n",
    "    if len(buf) != buf_maxlen:\n",
    "        a_t = env.action_space.sample()\n",
    "        x, _, _, _ = env.step(a_t)\n",
    "        buf.append(rgb2gray(x))\n",
    "        buf_idx.append(t)\n",
    "    else:\n",
    "        # phi_t is going to be the 4 most recent frames\n",
    "        phi_t = np.asarray([\n",
    "            buf[(t-1-3)%len(buf)], \n",
    "            buf[(t-1-2)%len(buf)], \n",
    "            buf[(t-1-1)%len(buf)], \n",
    "            buf[(t-1-0)%len(buf)]\n",
    "        ]).astype(\"float32\")\n",
    "        debug_t = [buf_idx[(t-1-3)%len(buf_idx)], \n",
    "                   buf_idx[(t-1-2)%len(buf_idx)], \n",
    "                   buf_idx[(t-1-1)%len(buf_idx)], \n",
    "                   buf_idx[(t-1-0)%len(buf_idx)]]\n",
    "        # with probability eps, select a random action\n",
    "        #a_t = env.action_space.sample()\n",
    "        a_t = eps_greedy_action(phi_t, eps=eps)\n",
    "        # execute action a_t in emulator and observe reward r_t and image x_t+1\n",
    "        x_t1, r_t, is_done, info = env.step(a_t)\n",
    "        # insert x_t+1 into the buffer, then grab the next\n",
    "        # 4 most recent frames\n",
    "        if not is_done:\n",
    "            buf[ (t) % len(buf) ] = rgb2gray(x_t1)\n",
    "            buf_idx[ (t) % len(buf) ] = t\n",
    "            phi_t1 = np.asarray([\n",
    "                buf[(t-3)%len(buf)], \n",
    "                buf[(t-2)%len(buf)], \n",
    "                buf[(t-1)%len(buf)], \n",
    "                buf[(t-0)%len(buf)]\n",
    "            ]).astype(\"float32\")\n",
    "            debug_t1 = [buf_idx[(t-3)%len(buf_idx)], \n",
    "                       buf_idx[(t-2)%len(buf_idx)], \n",
    "                       buf_idx[(t-1)%len(buf_idx)], \n",
    "                       buf_idx[(t-0)%len(buf_idx)]]\n",
    "        else:\n",
    "            phi_t1 = phi_t\n",
    "        #print debug_t, debug_t1\n",
    "        # add this tuple to the experience buffer\n",
    "        experience.append( {\"phi_t\":phi_t, \"a_t\":a_t, \"r_t\":r_t, \"phi_t1\":phi_t1, \"is_done\":is_done} )\n",
    "        \n",
    "        if len(experience) > mb_size:\n",
    "            # sample from random experience from the buffer\n",
    "            idxs = [i for i in range(0, len(experience))]\n",
    "            np.random.shuffle(idxs)\n",
    "            rand_transitions = [ experience[idx] for idx in idxs[0:mb_size] ]\n",
    "            phi_t1_minibatch = np.asarray(\n",
    "                [ rand_transitions[i][\"phi_t1\"] for i in range(len(rand_transitions)) ]).astype(\"float32\")\n",
    "            qvalues_minibatch = out_fn(phi_t1_minibatch)\n",
    "            max_qvalues_minibatch = np.max(qvalues_minibatch,axis=1)\n",
    "            y_minibatch = []\n",
    "            for i in range(qvalues_minibatch.shape[0]):\n",
    "                if rand_transitions[i][\"is_done\"]:\n",
    "                    y_minibatch.append([rand_transitions[i][\"r_t\"]])\n",
    "                else:\n",
    "                    y_minibatch.append([rand_transitions[i][\"r_t\"]+gamma*max_qvalues_minibatch[i]])\n",
    "            y_minibatch = np.asarray(y_minibatch).astype(\"float32\")\n",
    "\n",
    "            #print qvalues_minibatch\n",
    "            #print y_minibatch\n",
    "            \n",
    "            mask_minibatch = np.zeros(qvalues_minibatch.shape).astype(\"float32\")\n",
    "            for i in range(qvalues_minibatch.shape[0]):\n",
    "                mask_minibatch[ i, np.argmax(qvalues_minibatch[i]) ] = 1.\n",
    "                \n",
    "            #print mask_minibatch\n",
    "            \n",
    "            #print \"qvalues_minibatch\", qvalues_minibatch.shape\n",
    "            #print \"y_minibatch\", y_minibatch.shape\n",
    "            #print \"mask_minibatch\", mask_minibatch.shape\n",
    "            #print \"phi_t1_minibatch\", phi_t1_minibatch.shape\n",
    "            \n",
    "            this_loss = train_fn(phi_t1_minibatch, y_minibatch, mask_minibatch)\n",
    "            print this_loss\n",
    "            \n",
    "            #print test_fn(phi_t1_minibatch, mask_minibatch)\n",
    "            \n",
    "            #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.91\n",
      "0.82\n",
      "0.73\n",
      "0.64\n",
      "0.55\n",
      "0.46\n",
      "0.37\n",
      "0.28\n",
      "0.19\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "eps_dec_factor = (1 - 0.1) / 10\n",
    "for t in range(11):\n",
    "    curr_eps = 1 - (t*eps_dec_factor)\n",
    "    print curr_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999999999999998"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_max, eps_min, thresh = 1.0, 0.1, 1000000.0\n",
    "eps_dec_factor = (eps_max - eps_min) / thresh\n",
    "eps_max - ((1000*1000)*eps_dec_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "?set_all_param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DeepQ():\n",
    "    def _q_net(self, env):\n",
    "        height, width, nchannels = env.observation_space.shape\n",
    "        nchannels = 4 # we convert to black and white and use 4 prev frames\n",
    "        l_in = InputLayer((None, nchannels, height, width))\n",
    "        l_conv = Conv2DLayer(l_in, num_filters=32, filter_size=3, stride=2)\n",
    "        l_conv2 = Conv2DLayer(l_conv, num_filters=64, filter_size=3, stride=2)\n",
    "        l_conv3 = Conv2DLayer(l_conv2, num_filters=96, filter_size=3, stride=2)\n",
    "        l_conv4 = Conv2DLayer(l_conv3, num_filters=128, filter_size=3, stride=2)\n",
    "        l_dense = DenseLayer(l_conv4, num_units=env.action_space.n)\n",
    "        return l_dense\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.l_out = self._q_net(self.env)\n",
    "        # theano variables\n",
    "        X = T.tensor4('X')\n",
    "        y = T.fmatrix('y')\n",
    "        action_mask = T.fmatrix('action_mask')\n",
    "        # loss\n",
    "        net_out = get_output(self.l_out, X)\n",
    "        loss = ( y - (action_mask*net_out).sum(axis=1, keepdims=True) )**2\n",
    "        loss = loss.mean()\n",
    "        # theano functions\n",
    "        self.q_fn = theano.function([X], net_out)\n",
    "        params = get_all_params(self.l_out, trainable=True)\n",
    "        updates = lasagne.updates.rmsprop(loss, params, learning_rate=0.1)\n",
    "        self.train_fn = theano.function([X,y,action_mask], loss, updates=updates)\n",
    "    def _preprocess_frame(self, img):\n",
    "        return rgb2gray(img)\n",
    "    def _eps_greedy_action(self, phi_t, eps=0.1):\n",
    "        \"\"\"\n",
    "        phi_t: the pre-processed image for this time step\n",
    "        \"\"\"\n",
    "        if np.random.random() <= eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            action_dist = self.q_fn(phi_t[np.newaxis])\n",
    "            best_action = np.argmax(action_dist, axis=1)[0]\n",
    "            return best_action \n",
    "    def _save_as_pickle(self, arr, out_file):\n",
    "        with open(out_file,\"w\") as f:\n",
    "            pickle.dump(arr, f, pickle.HIGHEST_PROTOCOL)\n",
    "    def load_weights_from(self, in_file):\n",
    "        weights = pickle.load(open(in_file))\n",
    "        set_all_param_values(self.l_out, weights)   \n",
    "    def _save_weights_to(self, out_file):\n",
    "        self._save_as_pickle(get_all_param_values(self.l_out), out_file)\n",
    "    def train(self, \n",
    "              gamma=0.95,\n",
    "              eps=1.,\n",
    "              max_t=100000,\n",
    "              max_ep=100, \n",
    "              max_frames=None,\n",
    "              update_q=True,\n",
    "              batch_size=32,\n",
    "              save_experience_to=None,\n",
    "              save_outfile_to=None,\n",
    "              debug=False):\n",
    "        \"\"\"\n",
    "        :gamma: discount factor\n",
    "        :eps: initial eps factor for exploration\n",
    "        :max_t: maximum # of time steps per episode before it ends\n",
    "        :max_ep: maximum # of episodes before termination of this fn\n",
    "        :max_frames: maximum # of frames to see before termination of this fn\n",
    "        :update_q: backprop through Q fn\n",
    "        :batch_size: batch size for updates\n",
    "        :save_experience_to: save most recent experience to pkl file\n",
    "        :debug:\n",
    "        \"\"\"\n",
    "        f = open(save_outfile_to, \"wb\") if save_outfile_to != None else None\n",
    "        experience, experience_maxlen = [], 20000\n",
    "        tot_frames = 0\n",
    "        eps_max, eps_min, thresh = eps, 0.1, 1000000.0\n",
    "        eps_dec_factor = (eps_max - eps_min) / thresh\n",
    "        curr_eps = eps_max\n",
    "        for ep in range(max_ep):\n",
    "            losses = []\n",
    "            buf, buf_maxlen = [], 4\n",
    "            env.reset()\n",
    "            a_t = self.env.action_space.sample()\n",
    "            phi_t = None\n",
    "            buf_was_full = False\n",
    "            for t in range(max_t):\n",
    "                # with probability eps, select a random action a_t\n",
    "                # otherwise select it from the Q function\n",
    "                # NOTE: because we take a different action every 4\n",
    "                # frames, only re-assign a_t when the buf was previously\n",
    "                # full\n",
    "                if buf_was_full:\n",
    "                    # from deep-q paper: anneal eps from 1 to 0.1\n",
    "                    # over the course of 1m iterations\n",
    "                    if tot_frames <= thresh:\n",
    "                        curr_eps = eps_max - (tot_frames*eps_dec_factor)\n",
    "                    else:\n",
    "                        curr_eps = eps_min\n",
    "                    a_t = self._eps_greedy_action(phi_t, eps=curr_eps)\n",
    "                    buf_was_full = False\n",
    "                    #if debug:\n",
    "                    #    print \"%i: make new a_t\" % t\n",
    "                else:\n",
    "                    a_t = a_t\n",
    "                    #if debug:\n",
    "                    #    print \"%i: keep current a_t\" % t\n",
    "                    #print \"buf_was_not_Full\"\n",
    "                # execute action a_t in emulator and observe\n",
    "                # reward r_t and image x_t+1\n",
    "                x_t1, r_t, is_done, info = env.step(a_t)\n",
    "                buf.append(self._preprocess_frame(x_t1))\n",
    "                tot_frames += 1\n",
    "                #print len(buf)\n",
    "                # store transition (phi_t, a_t, r_t, phi_t1 into D)\n",
    "                # NOTE: this requires two conditions to be met:\n",
    "                # - that phi_t != None (it is None on the first execution of the\n",
    "                #   below if statement)\n",
    "                # - that phi_t exists (which is only the case when the buffer is full)\n",
    "                if len(buf) == buf_maxlen:\n",
    "                    phi_t1 = np.asarray(buf, dtype=\"float32\")\n",
    "                    if phi_t != None:\n",
    "                        tp = {\"phi_t\":phi_t, \"a_t\":a_t, \"r_t\":r_t, \"phi_t1\":phi_t1, \"is_done\":is_done}\n",
    "                        if len(experience) != experience_maxlen:\n",
    "                            experience.append(tp)\n",
    "                        else:\n",
    "                            experience[ tot_frames % len(experience) ] = tp   \n",
    "                    phi_t = phi_t1\n",
    "                    buf = []\n",
    "                    buf_was_full = True\n",
    "                    if is_done:\n",
    "                        out_str = \"episode %i took %i iterations, avg loss = %f, curr_eps = %f\" % \\\n",
    "                            (ep+1, t+1, np.mean(losses), curr_eps)\n",
    "                        print out_str\n",
    "                        if f != None:\n",
    "                            f.write(out_str + \"\\n\"); f.flush()\n",
    "                        #self._save_as_pickle(experience, \"/storeSSD/cbeckham/deleteme.pkl\")\n",
    "                        self._save_weights_to(\"weights.pkl\")\n",
    "                        break\n",
    "\n",
    "                    if len(experience) > batch_size and update_q:\n",
    "\n",
    "                        # sample from random experience from the buffer\n",
    "                        idxs = [i for i in range(0, len(experience))] # index into ring buffer\n",
    "                        np.random.shuffle(idxs)\n",
    "                        rand_transitions = \\\n",
    "                            [ experience[idx] for idx in idxs[0:batch_size] if experience[idx] != None ]\n",
    "                        phi_t1_minibatch = np.asarray(\n",
    "                            [ rand_transitions[i][\"phi_t1\"] for i in range(len(rand_transitions)) ]).astype(\"float32\")\n",
    "                        qvalues_minibatch = self.q_fn(phi_t1_minibatch)\n",
    "                        max_qvalues_minibatch = np.max(qvalues_minibatch,axis=1)\n",
    "                        y_minibatch = []\n",
    "                        for i in range(qvalues_minibatch.shape[0]):\n",
    "                            if rand_transitions[i][\"is_done\"]:\n",
    "                                y_minibatch.append([rand_transitions[i][\"r_t\"]])\n",
    "                            else:\n",
    "                                y_minibatch.append([rand_transitions[i][\"r_t\"]+gamma*max_qvalues_minibatch[i]])\n",
    "                        y_minibatch = np.asarray(y_minibatch).astype(\"float32\")\n",
    "                        #print qvalues_minibatch\n",
    "                        #print y_minibatch\n",
    "                        mask_minibatch = np.zeros(qvalues_minibatch.shape).astype(\"float32\")\n",
    "                        for i in range(qvalues_minibatch.shape[0]):\n",
    "                            mask_minibatch[ i, np.argmax(qvalues_minibatch[i]) ] = 1.\n",
    "                        #print mask_minibatch\n",
    "                        #print \"qvalues_minibatch\", qvalues_minibatch.shape\n",
    "                        #print \"y_minibatch\", y_minibatch.shape\n",
    "                        #print \"mask_minibatch\", mask_minibatch.shape\n",
    "                        #print \"phi_t1_minibatch\", phi_t1_minibatch.shape\n",
    "                        this_loss = self.train_fn(phi_t1_minibatch, y_minibatch, mask_minibatch)\n",
    "                        losses.append(this_loss)\n",
    "                    \n",
    "                if tot_frames >= max_frames:\n",
    "                    #return experience, losses\n",
    "                    return\n",
    "                    \n",
    "            #return losses\n",
    "        #return experience, losses\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 07:51:56,468] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "qq = DeepQ(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbeckham/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:115: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 took 1168 iterations, avg loss = 0.076770, curr_eps = 0.998952\n",
      "episode 2 took 1368 iterations, avg loss = 0.020711, curr_eps = 0.997721\n",
      "episode 3 took 1100 iterations, avg loss = 0.025091, curr_eps = 0.996731\n",
      "episode 4 took 1248 iterations, avg loss = 0.023814, curr_eps = 0.995608\n",
      "episode 5 took 1172 iterations, avg loss = 0.025043, curr_eps = 0.994553\n",
      "episode 6 took 1248 iterations, avg loss = 0.022408, curr_eps = 0.993430\n",
      "episode 7 took 1324 iterations, avg loss = 0.022443, curr_eps = 0.992238\n",
      "episode 8 took 1568 iterations, avg loss = 0.021899, curr_eps = 0.990827\n",
      "episode 9 took 1436 iterations, avg loss = 0.021735, curr_eps = 0.989535\n",
      "episode 10 took 1288 iterations, avg loss = 0.020249, curr_eps = 0.988376\n",
      "episode 11 took 1180 iterations, avg loss = 0.018707, curr_eps = 0.987314\n",
      "episode 12 took 1200 iterations, avg loss = 0.020485, curr_eps = 0.986234\n",
      "episode 13 took 1116 iterations, avg loss = 0.021920, curr_eps = 0.985229\n",
      "episode 14 took 1120 iterations, avg loss = 0.020049, curr_eps = 0.984221\n",
      "episode 15 took 1592 iterations, avg loss = 0.018734, curr_eps = 0.982788\n",
      "episode 16 took 1352 iterations, avg loss = 0.020030, curr_eps = 0.981572\n",
      "episode 17 took 1208 iterations, avg loss = 0.018169, curr_eps = 0.980484\n",
      "episode 18 took 1160 iterations, avg loss = 0.019788, curr_eps = 0.979440\n",
      "episode 19 took 1368 iterations, avg loss = 0.016496, curr_eps = 0.978209\n",
      "episode 20 took 1048 iterations, avg loss = 0.016284, curr_eps = 0.977266\n",
      "episode 21 took 1220 iterations, avg loss = 0.018503, curr_eps = 0.976168\n",
      "episode 22 took 1420 iterations, avg loss = 0.020569, curr_eps = 0.974890\n",
      "episode 23 took 1168 iterations, avg loss = 0.020189, curr_eps = 0.973839\n",
      "episode 24 took 1340 iterations, avg loss = 0.018619, curr_eps = 0.972633\n",
      "episode 25 took 1104 iterations, avg loss = 0.020341, curr_eps = 0.971639\n",
      "episode 26 took 1064 iterations, avg loss = 0.016156, curr_eps = 0.970682\n",
      "episode 27 took 1240 iterations, avg loss = 0.017496, curr_eps = 0.969566\n",
      "episode 28 took 1284 iterations, avg loss = 0.018262, curr_eps = 0.968410\n",
      "episode 29 took 1084 iterations, avg loss = 0.019560, curr_eps = 0.967434\n",
      "episode 30 took 1060 iterations, avg loss = 0.019531, curr_eps = 0.966480\n",
      "episode 31 took 1276 iterations, avg loss = 0.019556, curr_eps = 0.965332\n",
      "episode 32 took 1168 iterations, avg loss = 0.018471, curr_eps = 0.964281\n",
      "episode 33 took 1112 iterations, avg loss = 0.018276, curr_eps = 0.963280\n",
      "episode 34 took 1440 iterations, avg loss = 0.018454, curr_eps = 0.961984\n",
      "episode 35 took 1376 iterations, avg loss = 0.018130, curr_eps = 0.960746\n",
      "episode 36 took 1204 iterations, avg loss = 0.020833, curr_eps = 0.959662\n",
      "episode 37 took 1180 iterations, avg loss = 0.017007, curr_eps = 0.958600\n",
      "episode 38 took 1204 iterations, avg loss = 0.017396, curr_eps = 0.957516\n",
      "episode 39 took 1012 iterations, avg loss = 0.017237, curr_eps = 0.956606\n",
      "episode 40 took 1100 iterations, avg loss = 0.019503, curr_eps = 0.955616\n",
      "episode 41 took 1300 iterations, avg loss = 0.018519, curr_eps = 0.954446\n",
      "episode 42 took 1292 iterations, avg loss = 0.019992, curr_eps = 0.953283\n",
      "episode 43 took 1272 iterations, avg loss = 0.017744, curr_eps = 0.952138\n",
      "episode 44 took 1164 iterations, avg loss = 0.017457, curr_eps = 0.951090\n",
      "episode 45 took 1180 iterations, avg loss = 0.018070, curr_eps = 0.950028\n",
      "episode 46 took 1200 iterations, avg loss = 0.016827, curr_eps = 0.948948\n",
      "episode 47 took 1136 iterations, avg loss = 0.020208, curr_eps = 0.947926\n",
      "episode 48 took 1404 iterations, avg loss = 0.017589, curr_eps = 0.946662\n",
      "episode 49 took 1016 iterations, avg loss = 0.016922, curr_eps = 0.945748\n",
      "episode 50 took 1108 iterations, avg loss = 0.018456, curr_eps = 0.944751\n",
      "episode 51 took 1036 iterations, avg loss = 0.018774, curr_eps = 0.943818\n",
      "episode 52 took 1204 iterations, avg loss = 0.018333, curr_eps = 0.942735\n",
      "episode 53 took 1472 iterations, avg loss = 0.017285, curr_eps = 0.941410\n",
      "episode 54 took 1116 iterations, avg loss = 0.019222, curr_eps = 0.940406\n",
      "episode 55 took 1064 iterations, avg loss = 0.014033, curr_eps = 0.939448\n",
      "episode 56 took 1116 iterations, avg loss = 0.019559, curr_eps = 0.938444\n",
      "episode 57 took 1060 iterations, avg loss = 0.019176, curr_eps = 0.937490\n",
      "episode 58 took 1180 iterations, avg loss = 0.017645, curr_eps = 0.936428\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-305-6489272f3de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m qq.train(max_t=100000, max_ep=100000, \n\u001b[0;32m----> 2\u001b[0;31m          max_frames=10000000, update_q=True, debug=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-303-a1860fafb641>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gamma, eps, max_t, max_ep, max_frames, update_q, batch_size, save_experience_to, save_outfile_to, debug)\u001b[0m\n\u001b[1;32m    138\u001b[0m                         \u001b[0mrand_transitions\u001b[0m \u001b[0;34m=\u001b[0m                             \u001b[0;34m[\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                         phi_t1_minibatch = np.asarray(\n\u001b[0;32m--> 140\u001b[0;31m                             [ rand_transitions[i][\"phi_t1\"] for i in range(len(rand_transitions)) ]).astype(\"float32\")\n\u001b[0m\u001b[1;32m    141\u001b[0m                         \u001b[0mqvalues_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi_t1_minibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                         \u001b[0mmax_qvalues_minibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues_minibatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qq.train(max_t=100000, max_ep=100000, \n",
    "         max_frames=10000000, update_q=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-19 23:18:05,317] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "qq = DeepQ(env)\n",
    "qq.load_weights_from(\"weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbeckham/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:118: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 took 1088 iterations, avg loss = 0.020614, curr_eps = 0.929100\n",
      "episode 2 took 1140 iterations, avg loss = 0.017055, curr_eps = 0.928154\n",
      "episode 3 took 1096 iterations, avg loss = 0.019803, curr_eps = 0.927244\n",
      "episode 4 took 1304 iterations, avg loss = 0.017019, curr_eps = 0.926162\n",
      "episode 5 took 1424 iterations, avg loss = 0.020158, curr_eps = 0.924980\n",
      "episode 6 took 1256 iterations, avg loss = 0.019469, curr_eps = 0.923938\n",
      "episode 7 took 1368 iterations, avg loss = 0.018237, curr_eps = 0.922802\n",
      "episode 8 took 1412 iterations, avg loss = 0.019975, curr_eps = 0.921630\n",
      "episode 9 took 1088 iterations, avg loss = 0.017066, curr_eps = 0.920727\n",
      "episode 10 took 1424 iterations, avg loss = 0.017958, curr_eps = 0.919545\n",
      "episode 11 took 1132 iterations, avg loss = 0.019393, curr_eps = 0.918606\n",
      "episode 12 took 1140 iterations, avg loss = 0.020026, curr_eps = 0.917660\n",
      "episode 13 took 1084 iterations, avg loss = 0.020718, curr_eps = 0.916760\n",
      "episode 14 took 1204 iterations, avg loss = 0.019896, curr_eps = 0.915761\n",
      "episode 15 took 1448 iterations, avg loss = 0.018006, curr_eps = 0.914559\n",
      "episode 16 took 1448 iterations, avg loss = 0.017746, curr_eps = 0.913357\n",
      "episode 17 took 1256 iterations, avg loss = 0.018570, curr_eps = 0.912314\n",
      "episode 18 took 1096 iterations, avg loss = 0.018658, curr_eps = 0.911405\n",
      "episode 19 took 1024 iterations, avg loss = 0.018627, curr_eps = 0.910555\n",
      "episode 20 took 1116 iterations, avg loss = 0.016524, curr_eps = 0.909628\n",
      "episode 21 took 1220 iterations, avg loss = 0.016859, curr_eps = 0.908616\n",
      "episode 22 took 1176 iterations, avg loss = 0.020158, curr_eps = 0.907640\n",
      "episode 23 took 1252 iterations, avg loss = 0.019030, curr_eps = 0.906601\n",
      "episode 24 took 1072 iterations, avg loss = 0.017790, curr_eps = 0.905711\n",
      "episode 25 took 1124 iterations, avg loss = 0.019866, curr_eps = 0.904778\n",
      "episode 26 took 1316 iterations, avg loss = 0.019150, curr_eps = 0.903686\n",
      "episode 27 took 1280 iterations, avg loss = 0.017731, curr_eps = 0.902623\n",
      "episode 28 took 1260 iterations, avg loss = 0.018909, curr_eps = 0.901577\n",
      "episode 29 took 1156 iterations, avg loss = 0.018989, curr_eps = 0.900618\n",
      "episode 30 took 1228 iterations, avg loss = 0.020016, curr_eps = 0.899599\n",
      "episode 31 took 1064 iterations, avg loss = 0.021580, curr_eps = 0.898716\n",
      "episode 32 took 1284 iterations, avg loss = 0.019336, curr_eps = 0.897650\n",
      "episode 33 took 1376 iterations, avg loss = 0.019588, curr_eps = 0.896508\n",
      "episode 34 took 1320 iterations, avg loss = 0.021752, curr_eps = 0.895412\n",
      "episode 35 took 1192 iterations, avg loss = 0.016835, curr_eps = 0.894423\n",
      "episode 36 took 1148 iterations, avg loss = 0.020870, curr_eps = 0.893470\n",
      "episode 37 took 1156 iterations, avg loss = 0.019423, curr_eps = 0.892511\n",
      "episode 38 took 1084 iterations, avg loss = 0.018171, curr_eps = 0.891611\n",
      "episode 39 took 1052 iterations, avg loss = 0.019919, curr_eps = 0.890738\n",
      "episode 40 took 1212 iterations, avg loss = 0.021006, curr_eps = 0.889732\n",
      "episode 41 took 1316 iterations, avg loss = 0.020865, curr_eps = 0.888639\n",
      "episode 42 took 1076 iterations, avg loss = 0.020173, curr_eps = 0.887746\n",
      "episode 43 took 1532 iterations, avg loss = 0.017261, curr_eps = 0.886475\n",
      "episode 44 took 1116 iterations, avg loss = 0.019897, curr_eps = 0.885549\n",
      "episode 45 took 1304 iterations, avg loss = 0.019808, curr_eps = 0.884466\n",
      "episode 46 took 1120 iterations, avg loss = 0.016577, curr_eps = 0.883537\n",
      "episode 47 took 1124 iterations, avg loss = 0.022321, curr_eps = 0.882604\n",
      "episode 48 took 1276 iterations, avg loss = 0.020145, curr_eps = 0.881545\n",
      "episode 49 took 1128 iterations, avg loss = 0.018794, curr_eps = 0.880608\n",
      "episode 50 took 1144 iterations, avg loss = 0.019079, curr_eps = 0.879659\n",
      "episode 51 took 1424 iterations, avg loss = 0.018926, curr_eps = 0.878477\n",
      "episode 52 took 1016 iterations, avg loss = 0.019392, curr_eps = 0.877634\n",
      "episode 53 took 1532 iterations, avg loss = 0.018979, curr_eps = 0.876362\n",
      "episode 54 took 1272 iterations, avg loss = 0.020702, curr_eps = 0.875306\n",
      "episode 55 took 1404 iterations, avg loss = 0.016161, curr_eps = 0.874141\n",
      "episode 56 took 1232 iterations, avg loss = 0.020867, curr_eps = 0.873118\n",
      "episode 57 took 1068 iterations, avg loss = 0.016330, curr_eps = 0.872232\n",
      "episode 58 took 1144 iterations, avg loss = 0.018202, curr_eps = 0.871282\n",
      "episode 59 took 1104 iterations, avg loss = 0.020568, curr_eps = 0.870366\n",
      "episode 60 took 1260 iterations, avg loss = 0.018213, curr_eps = 0.869320\n",
      "episode 61 took 1104 iterations, avg loss = 0.016591, curr_eps = 0.868404\n",
      "episode 62 took 1376 iterations, avg loss = 0.019315, curr_eps = 0.867262\n",
      "episode 63 took 1428 iterations, avg loss = 0.019136, curr_eps = 0.866077\n",
      "episode 64 took 1116 iterations, avg loss = 0.017648, curr_eps = 0.865150\n",
      "episode 65 took 1296 iterations, avg loss = 0.017995, curr_eps = 0.864075\n",
      "episode 66 took 1320 iterations, avg loss = 0.018237, curr_eps = 0.862979\n",
      "episode 67 took 1188 iterations, avg loss = 0.017736, curr_eps = 0.861993\n",
      "episode 68 took 1464 iterations, avg loss = 0.016610, curr_eps = 0.860778\n",
      "episode 69 took 1104 iterations, avg loss = 0.018295, curr_eps = 0.859862\n",
      "episode 70 took 1024 iterations, avg loss = 0.018015, curr_eps = 0.859012\n",
      "episode 71 took 1052 iterations, avg loss = 0.020157, curr_eps = 0.858139\n",
      "episode 72 took 1184 iterations, avg loss = 0.019386, curr_eps = 0.857156\n",
      "episode 73 took 1168 iterations, avg loss = 0.018363, curr_eps = 0.856186\n",
      "episode 74 took 1024 iterations, avg loss = 0.015931, curr_eps = 0.855337\n",
      "episode 75 took 1272 iterations, avg loss = 0.017843, curr_eps = 0.854281\n",
      "episode 76 took 1160 iterations, avg loss = 0.018166, curr_eps = 0.853318\n",
      "episode 77 took 1068 iterations, avg loss = 0.020324, curr_eps = 0.852432\n",
      "episode 78 took 1104 iterations, avg loss = 0.018409, curr_eps = 0.851515\n",
      "episode 79 took 1080 iterations, avg loss = 0.015915, curr_eps = 0.850619\n",
      "episode 80 took 1088 iterations, avg loss = 0.018220, curr_eps = 0.849716\n",
      "episode 81 took 1136 iterations, avg loss = 0.017005, curr_eps = 0.848773\n",
      "episode 82 took 1100 iterations, avg loss = 0.021099, curr_eps = 0.847860\n",
      "episode 83 took 1276 iterations, avg loss = 0.019949, curr_eps = 0.846801\n",
      "episode 84 took 1352 iterations, avg loss = 0.019288, curr_eps = 0.845679\n",
      "episode 85 took 1284 iterations, avg loss = 0.018164, curr_eps = 0.844613\n",
      "episode 86 took 1148 iterations, avg loss = 0.019012, curr_eps = 0.843660\n",
      "episode 87 took 1124 iterations, avg loss = 0.019085, curr_eps = 0.842727\n",
      "episode 88 took 1488 iterations, avg loss = 0.017773, curr_eps = 0.841492\n",
      "episode 89 took 1280 iterations, avg loss = 0.017437, curr_eps = 0.840430\n",
      "episode 90 took 1324 iterations, avg loss = 0.018371, curr_eps = 0.839331\n",
      "episode 91 took 1108 iterations, avg loss = 0.019928, curr_eps = 0.838411\n",
      "episode 92 took 1552 iterations, avg loss = 0.019622, curr_eps = 0.837123\n",
      "episode 93 took 1180 iterations, avg loss = 0.020408, curr_eps = 0.836144\n",
      "episode 94 took 1164 iterations, avg loss = 0.017996, curr_eps = 0.835177\n",
      "episode 95 took 1304 iterations, avg loss = 0.018846, curr_eps = 0.834095\n",
      "episode 96 took 1112 iterations, avg loss = 0.019066, curr_eps = 0.833172\n",
      "episode 97 took 1032 iterations, avg loss = 0.018604, curr_eps = 0.832316\n",
      "episode 98 took 1036 iterations, avg loss = 0.021439, curr_eps = 0.831456\n",
      "episode 99 took 1444 iterations, avg loss = 0.020747, curr_eps = 0.830257\n",
      "episode 100 took 1164 iterations, avg loss = 0.021444, curr_eps = 0.829291\n",
      "episode 101 took 1092 iterations, avg loss = 0.020221, curr_eps = 0.828385\n",
      "episode 102 took 1092 iterations, avg loss = 0.018957, curr_eps = 0.827478\n",
      "episode 103 took 1460 iterations, avg loss = 0.018029, curr_eps = 0.826267\n",
      "episode 104 took 1308 iterations, avg loss = 0.020993, curr_eps = 0.825181\n",
      "episode 105 took 1236 iterations, avg loss = 0.017248, curr_eps = 0.824155\n",
      "episode 106 took 1304 iterations, avg loss = 0.021635, curr_eps = 0.823073\n",
      "episode 107 took 1164 iterations, avg loss = 0.019289, curr_eps = 0.822107\n",
      "episode 108 took 1340 iterations, avg loss = 0.018525, curr_eps = 0.820994\n",
      "episode 109 took 1180 iterations, avg loss = 0.019664, curr_eps = 0.820015\n",
      "episode 110 took 1316 iterations, avg loss = 0.019722, curr_eps = 0.818923\n",
      "episode 111 took 1224 iterations, avg loss = 0.019057, curr_eps = 0.817907\n",
      "episode 112 took 1304 iterations, avg loss = 0.020288, curr_eps = 0.816825\n",
      "episode 113 took 1124 iterations, avg loss = 0.019085, curr_eps = 0.815892\n",
      "episode 114 took 1072 iterations, avg loss = 0.019312, curr_eps = 0.815002\n",
      "episode 115 took 1244 iterations, avg loss = 0.018246, curr_eps = 0.813969\n",
      "episode 116 took 1188 iterations, avg loss = 0.020165, curr_eps = 0.812983\n",
      "episode 117 took 1236 iterations, avg loss = 0.018466, curr_eps = 0.811957\n",
      "episode 118 took 1488 iterations, avg loss = 0.019458, curr_eps = 0.810722\n",
      "episode 119 took 1264 iterations, avg loss = 0.020139, curr_eps = 0.809673\n",
      "episode 120 took 1212 iterations, avg loss = 0.018419, curr_eps = 0.808667\n",
      "episode 121 took 1396 iterations, avg loss = 0.015535, curr_eps = 0.807509\n",
      "episode 122 took 1248 iterations, avg loss = 0.020699, curr_eps = 0.806473\n",
      "episode 123 took 1248 iterations, avg loss = 0.021202, curr_eps = 0.805437\n",
      "episode 124 took 1132 iterations, avg loss = 0.017176, curr_eps = 0.804497\n",
      "episode 125 took 1016 iterations, avg loss = 0.019145, curr_eps = 0.803654\n",
      "episode 126 took 1308 iterations, avg loss = 0.016775, curr_eps = 0.802568\n",
      "episode 127 took 1344 iterations, avg loss = 0.019123, curr_eps = 0.801453\n",
      "episode 128 took 1252 iterations, avg loss = 0.018129, curr_eps = 0.800414\n",
      "episode 129 took 1120 iterations, avg loss = 0.016913, curr_eps = 0.799484\n",
      "episode 130 took 1328 iterations, avg loss = 0.017088, curr_eps = 0.798382\n",
      "episode 131 took 1492 iterations, avg loss = 0.018565, curr_eps = 0.797144\n",
      "episode 132 took 1336 iterations, avg loss = 0.020646, curr_eps = 0.796035\n",
      "episode 133 took 1280 iterations, avg loss = 0.016164, curr_eps = 0.794972\n",
      "episode 134 took 1212 iterations, avg loss = 0.015625, curr_eps = 0.793966\n",
      "episode 135 took 1088 iterations, avg loss = 0.017528, curr_eps = 0.793063\n",
      "episode 136 took 1028 iterations, avg loss = 0.016724, curr_eps = 0.792210\n",
      "episode 137 took 1112 iterations, avg loss = 0.021322, curr_eps = 0.791287\n",
      "episode 138 took 1140 iterations, avg loss = 0.018266, curr_eps = 0.790341\n",
      "episode 139 took 1016 iterations, avg loss = 0.016922, curr_eps = 0.789498\n",
      "episode 140 took 1160 iterations, avg loss = 0.016652, curr_eps = 0.788535\n",
      "episode 141 took 1020 iterations, avg loss = 0.022146, curr_eps = 0.787688\n",
      "episode 142 took 1308 iterations, avg loss = 0.016392, curr_eps = 0.786603\n",
      "episode 143 took 1344 iterations, avg loss = 0.017817, curr_eps = 0.785487\n",
      "episode 144 took 1200 iterations, avg loss = 0.020798, curr_eps = 0.784491\n",
      "episode 145 took 1128 iterations, avg loss = 0.019017, curr_eps = 0.783555\n",
      "episode 146 took 1452 iterations, avg loss = 0.018128, curr_eps = 0.782350\n",
      "episode 147 took 1212 iterations, avg loss = 0.020178, curr_eps = 0.781344\n",
      "episode 148 took 1224 iterations, avg loss = 0.017520, curr_eps = 0.780328\n",
      "episode 149 took 1092 iterations, avg loss = 0.019072, curr_eps = 0.779421\n",
      "episode 150 took 1248 iterations, avg loss = 0.021503, curr_eps = 0.778386\n",
      "episode 151 took 1300 iterations, avg loss = 0.019676, curr_eps = 0.777307\n",
      "episode 152 took 1100 iterations, avg loss = 0.016880, curr_eps = 0.776394\n",
      "episode 153 took 1004 iterations, avg loss = 0.018750, curr_eps = 0.775560\n",
      "episode 154 took 1212 iterations, avg loss = 0.019454, curr_eps = 0.774554\n",
      "episode 155 took 1140 iterations, avg loss = 0.020246, curr_eps = 0.773608\n",
      "episode 156 took 1024 iterations, avg loss = 0.018382, curr_eps = 0.772758\n",
      "episode 157 took 1124 iterations, avg loss = 0.019978, curr_eps = 0.771825\n",
      "episode 158 took 1108 iterations, avg loss = 0.020380, curr_eps = 0.770906\n",
      "episode 159 took 1404 iterations, avg loss = 0.016161, curr_eps = 0.769740\n",
      "episode 160 took 1072 iterations, avg loss = 0.018375, curr_eps = 0.768851\n",
      "episode 161 took 1396 iterations, avg loss = 0.018499, curr_eps = 0.767692\n",
      "episode 162 took 1016 iterations, avg loss = 0.017540, curr_eps = 0.766849\n",
      "episode 163 took 1124 iterations, avg loss = 0.016629, curr_eps = 0.765916\n",
      "episode 164 took 1532 iterations, avg loss = 0.017752, curr_eps = 0.764644\n",
      "episode 165 took 1008 iterations, avg loss = 0.016683, curr_eps = 0.763807\n",
      "episode 166 took 1932 iterations, avg loss = 0.018478, curr_eps = 0.762204\n",
      "episode 167 took 1108 iterations, avg loss = 0.016531, curr_eps = 0.761284\n",
      "episode 168 took 1252 iterations, avg loss = 0.018129, curr_eps = 0.760245\n",
      "episode 169 took 1448 iterations, avg loss = 0.015928, curr_eps = 0.759043\n",
      "episode 170 took 1200 iterations, avg loss = 0.016095, curr_eps = 0.758047\n",
      "episode 171 took 1092 iterations, avg loss = 0.018267, curr_eps = 0.757141\n",
      "episode 172 took 1272 iterations, avg loss = 0.017547, curr_eps = 0.756085\n",
      "episode 173 took 1176 iterations, avg loss = 0.018878, curr_eps = 0.755109\n",
      "episode 174 took 1092 iterations, avg loss = 0.017923, curr_eps = 0.754203\n",
      "episode 175 took 1104 iterations, avg loss = 0.018295, curr_eps = 0.753286\n",
      "episode 176 took 1448 iterations, avg loss = 0.019650, curr_eps = 0.752085\n",
      "episode 177 took 1324 iterations, avg loss = 0.018087, curr_eps = 0.750986\n",
      "episode 178 took 1016 iterations, avg loss = 0.018034, curr_eps = 0.750142\n",
      "episode 179 took 1068 iterations, avg loss = 0.015273, curr_eps = 0.749256\n",
      "episode 180 took 1324 iterations, avg loss = 0.016761, curr_eps = 0.748157\n",
      "episode 181 took 1072 iterations, avg loss = 0.018024, curr_eps = 0.747267\n",
      "episode 182 took 1144 iterations, avg loss = 0.018860, curr_eps = 0.746318\n",
      "episode 183 took 1096 iterations, avg loss = 0.019002, curr_eps = 0.745408\n",
      "episode 184 took 1112 iterations, avg loss = 0.018389, curr_eps = 0.744485\n",
      "episode 185 took 1448 iterations, avg loss = 0.019304, curr_eps = 0.743283\n",
      "episode 186 took 1184 iterations, avg loss = 0.019809, curr_eps = 0.742300\n",
      "episode 187 took 1040 iterations, avg loss = 0.018219, curr_eps = 0.741437\n",
      "episode 188 took 1008 iterations, avg loss = 0.018053, curr_eps = 0.740601\n",
      "episode 189 took 1104 iterations, avg loss = 0.019205, curr_eps = 0.739684\n",
      "episode 190 took 1264 iterations, avg loss = 0.019544, curr_eps = 0.738635\n",
      "episode 191 took 1060 iterations, avg loss = 0.019768, curr_eps = 0.737755\n",
      "episode 192 took 1100 iterations, avg loss = 0.019389, curr_eps = 0.736842\n",
      "episode 193 took 1040 iterations, avg loss = 0.018460, curr_eps = 0.735979\n",
      "episode 194 took 1012 iterations, avg loss = 0.018229, curr_eps = 0.735139\n",
      "episode 195 took 1268 iterations, avg loss = 0.016812, curr_eps = 0.734087\n",
      "episode 196 took 1084 iterations, avg loss = 0.018866, curr_eps = 0.733187\n",
      "episode 197 took 1092 iterations, avg loss = 0.019072, curr_eps = 0.732281\n",
      "episode 198 took 1180 iterations, avg loss = 0.017326, curr_eps = 0.731301\n",
      "episode 199 took 1196 iterations, avg loss = 0.018561, curr_eps = 0.730309\n",
      "episode 200 took 1184 iterations, avg loss = 0.015678, curr_eps = 0.729326\n",
      "episode 201 took 1100 iterations, avg loss = 0.016423, curr_eps = 0.728413\n",
      "episode 202 took 1196 iterations, avg loss = 0.017722, curr_eps = 0.727420\n",
      "episode 203 took 1160 iterations, avg loss = 0.016869, curr_eps = 0.726457\n",
      "episode 204 took 1040 iterations, avg loss = 0.016771, curr_eps = 0.725594\n",
      "episode 205 took 1204 iterations, avg loss = 0.017708, curr_eps = 0.724595\n",
      "episode 206 took 1288 iterations, avg loss = 0.017718, curr_eps = 0.723526\n",
      "episode 207 took 1172 iterations, avg loss = 0.018943, curr_eps = 0.722553\n",
      "episode 208 took 1288 iterations, avg loss = 0.016647, curr_eps = 0.721484\n",
      "episode 209 took 1160 iterations, avg loss = 0.017301, curr_eps = 0.720521\n",
      "episode 210 took 1192 iterations, avg loss = 0.018834, curr_eps = 0.719532\n",
      "episode 211 took 1088 iterations, avg loss = 0.018681, curr_eps = 0.718629\n",
      "episode 212 took 1176 iterations, avg loss = 0.017278, curr_eps = 0.717653\n",
      "episode 213 took 1024 iterations, avg loss = 0.017157, curr_eps = 0.716803\n",
      "episode 214 took 1176 iterations, avg loss = 0.018025, curr_eps = 0.715827\n",
      "episode 215 took 1392 iterations, avg loss = 0.017921, curr_eps = 0.714671\n",
      "episode 216 took 1332 iterations, avg loss = 0.018166, curr_eps = 0.713566\n",
      "episode 217 took 1176 iterations, avg loss = 0.017172, curr_eps = 0.712590\n",
      "episode 218 took 1120 iterations, avg loss = 0.018481, curr_eps = 0.711660\n",
      "episode 219 took 1272 iterations, avg loss = 0.017843, curr_eps = 0.710604\n",
      "episode 220 took 1136 iterations, avg loss = 0.020208, curr_eps = 0.709662\n",
      "episode 221 took 1508 iterations, avg loss = 0.017287, curr_eps = 0.708410\n",
      "episode 222 took 1408 iterations, avg loss = 0.016916, curr_eps = 0.707241\n",
      "episode 223 took 1200 iterations, avg loss = 0.019440, curr_eps = 0.706245\n",
      "episode 224 took 1008 iterations, avg loss = 0.019547, curr_eps = 0.705409\n",
      "episode 225 took 1036 iterations, avg loss = 0.018290, curr_eps = 0.704549\n",
      "episode 226 took 1216 iterations, avg loss = 0.018152, curr_eps = 0.703539\n",
      "episode 227 took 1016 iterations, avg loss = 0.020010, curr_eps = 0.702696\n",
      "episode 228 took 1100 iterations, avg loss = 0.020073, curr_eps = 0.701783\n",
      "episode 229 took 1116 iterations, avg loss = 0.021133, curr_eps = 0.700857\n",
      "episode 230 took 1464 iterations, avg loss = 0.018408, curr_eps = 0.699642\n",
      "episode 231 took 1104 iterations, avg loss = 0.016705, curr_eps = 0.698725\n",
      "episode 232 took 1056 iterations, avg loss = 0.018180, curr_eps = 0.697849\n",
      "episode 233 took 1100 iterations, avg loss = 0.015625, curr_eps = 0.696936\n",
      "episode 234 took 1224 iterations, avg loss = 0.015676, curr_eps = 0.695920\n",
      "episode 235 took 1364 iterations, avg loss = 0.019577, curr_eps = 0.694788\n",
      "episode 236 took 1104 iterations, avg loss = 0.018182, curr_eps = 0.693872\n",
      "episode 237 took 1064 iterations, avg loss = 0.017807, curr_eps = 0.692989\n",
      "episode 238 took 1144 iterations, avg loss = 0.017215, curr_eps = 0.692039\n",
      "episode 239 took 1028 iterations, avg loss = 0.020508, curr_eps = 0.691186\n",
      "episode 240 took 1132 iterations, avg loss = 0.017287, curr_eps = 0.690246\n",
      "episode 241 took 1108 iterations, avg loss = 0.016644, curr_eps = 0.689327\n",
      "episode 242 took 1208 iterations, avg loss = 0.018272, curr_eps = 0.688324\n",
      "episode 243 took 1180 iterations, avg loss = 0.019239, curr_eps = 0.687345\n",
      "episode 244 took 1356 iterations, avg loss = 0.022189, curr_eps = 0.686219\n",
      "episode 245 took 1192 iterations, avg loss = 0.018834, curr_eps = 0.685230\n",
      "episode 246 took 1320 iterations, avg loss = 0.017667, curr_eps = 0.684134\n",
      "episode 247 took 1028 iterations, avg loss = 0.021973, curr_eps = 0.683281\n",
      "episode 248 took 1052 iterations, avg loss = 0.020992, curr_eps = 0.682408\n",
      "episode 249 took 1108 iterations, avg loss = 0.021626, curr_eps = 0.681488\n",
      "episode 250 took 1200 iterations, avg loss = 0.021426, curr_eps = 0.680492\n",
      "episode 251 took 1052 iterations, avg loss = 0.016579, curr_eps = 0.679619\n",
      "episode 252 took 1176 iterations, avg loss = 0.018238, curr_eps = 0.678643\n",
      "episode 253 took 1032 iterations, avg loss = 0.018969, curr_eps = 0.677786\n",
      "episode 254 took 1256 iterations, avg loss = 0.019469, curr_eps = 0.676744\n",
      "episode 255 took 1188 iterations, avg loss = 0.020798, curr_eps = 0.675758\n",
      "episode 256 took 1424 iterations, avg loss = 0.019014, curr_eps = 0.674576\n",
      "episode 257 took 1024 iterations, avg loss = 0.018873, curr_eps = 0.673726\n",
      "episode 258 took 1312 iterations, avg loss = 0.017775, curr_eps = 0.672637\n",
      "episode 259 took 1284 iterations, avg loss = 0.018457, curr_eps = 0.671571\n",
      "episode 260 took 1052 iterations, avg loss = 0.019442, curr_eps = 0.670698\n",
      "episode 261 took 1152 iterations, avg loss = 0.019817, curr_eps = 0.669742\n",
      "episode 262 took 1188 iterations, avg loss = 0.019742, curr_eps = 0.668756\n",
      "episode 263 took 1372 iterations, avg loss = 0.016721, curr_eps = 0.667617\n",
      "episode 264 took 1008 iterations, avg loss = 0.018800, curr_eps = 0.666780\n",
      "episode 265 took 1000 iterations, avg loss = 0.016441, curr_eps = 0.665950\n",
      "episode 266 took 1216 iterations, avg loss = 0.017430, curr_eps = 0.664941\n",
      "episode 267 took 1064 iterations, avg loss = 0.018396, curr_eps = 0.664058\n",
      "episode 268 took 1168 iterations, avg loss = 0.018900, curr_eps = 0.663089\n",
      "episode 269 took 1252 iterations, avg loss = 0.020032, curr_eps = 0.662049\n",
      "episode 270 took 1116 iterations, avg loss = 0.017761, curr_eps = 0.661123\n",
      "episode 271 took 1264 iterations, avg loss = 0.018849, curr_eps = 0.660074\n",
      "episode 272 took 1176 iterations, avg loss = 0.019838, curr_eps = 0.659098\n",
      "episode 273 took 1016 iterations, avg loss = 0.017416, curr_eps = 0.658255\n",
      "episode 274 took 1168 iterations, avg loss = 0.016753, curr_eps = 0.657285\n",
      "episode 275 took 1108 iterations, avg loss = 0.020154, curr_eps = 0.656366\n",
      "episode 276 took 1280 iterations, avg loss = 0.016458, curr_eps = 0.655303\n",
      "episode 277 took 1216 iterations, avg loss = 0.017327, curr_eps = 0.654294\n",
      "episode 278 took 1488 iterations, avg loss = 0.020047, curr_eps = 0.653059\n",
      "episode 279 took 1060 iterations, avg loss = 0.017164, curr_eps = 0.652179\n",
      "episode 280 took 1148 iterations, avg loss = 0.019231, curr_eps = 0.651226\n",
      "episode 281 took 1132 iterations, avg loss = 0.019060, curr_eps = 0.650287\n",
      "episode 282 took 1012 iterations, avg loss = 0.020337, curr_eps = 0.649447\n",
      "episode 283 took 1432 iterations, avg loss = 0.019170, curr_eps = 0.648258\n",
      "episode 284 took 1120 iterations, avg loss = 0.017809, curr_eps = 0.647329\n",
      "episode 285 took 1056 iterations, avg loss = 0.020081, curr_eps = 0.646452\n",
      "episode 286 took 1096 iterations, avg loss = 0.018773, curr_eps = 0.645542\n",
      "episode 287 took 1172 iterations, avg loss = 0.019799, curr_eps = 0.644570\n",
      "episode 288 took 1132 iterations, avg loss = 0.018617, curr_eps = 0.643630\n",
      "episode 289 took 1300 iterations, avg loss = 0.018229, curr_eps = 0.642551\n",
      "episode 290 took 1020 iterations, avg loss = 0.017594, curr_eps = 0.641704\n",
      "episode 291 took 1076 iterations, avg loss = 0.018773, curr_eps = 0.640811\n",
      "episode 292 took 1148 iterations, avg loss = 0.017592, curr_eps = 0.639859\n",
      "episode 293 took 1364 iterations, avg loss = 0.016820, curr_eps = 0.638726\n",
      "episode 294 took 1008 iterations, avg loss = 0.020045, curr_eps = 0.637890\n",
      "episode 295 took 1192 iterations, avg loss = 0.018098, curr_eps = 0.636900\n",
      "episode 296 took 1172 iterations, avg loss = 0.018193, curr_eps = 0.635928\n",
      "episode 297 took 1120 iterations, avg loss = 0.018593, curr_eps = 0.634998\n",
      "episode 298 took 1292 iterations, avg loss = 0.018731, curr_eps = 0.633926\n",
      "episode 299 took 1232 iterations, avg loss = 0.019442, curr_eps = 0.632903\n",
      "episode 300 took 1108 iterations, avg loss = 0.018569, curr_eps = 0.631984\n",
      "episode 301 took 1200 iterations, avg loss = 0.020589, curr_eps = 0.630988\n",
      "episode 302 took 1288 iterations, avg loss = 0.019470, curr_eps = 0.629918\n",
      "episode 303 took 1108 iterations, avg loss = 0.017889, curr_eps = 0.628999\n",
      "episode 304 took 1012 iterations, avg loss = 0.017237, curr_eps = 0.628159\n",
      "episode 305 took 1344 iterations, avg loss = 0.018750, curr_eps = 0.627043\n",
      "episode 306 took 1336 iterations, avg loss = 0.022335, curr_eps = 0.625934\n",
      "episode 307 took 1096 iterations, avg loss = 0.018201, curr_eps = 0.625025\n",
      "episode 308 took 1032 iterations, avg loss = 0.018604, curr_eps = 0.624168\n",
      "episode 309 took 1184 iterations, avg loss = 0.019597, curr_eps = 0.623186\n",
      "episode 310 took 1196 iterations, avg loss = 0.015520, curr_eps = 0.622193\n",
      "episode 311 took 1256 iterations, avg loss = 0.018870, curr_eps = 0.621150\n",
      "episode 312 took 1264 iterations, avg loss = 0.018651, curr_eps = 0.620101\n",
      "episode 313 took 1112 iterations, avg loss = 0.019517, curr_eps = 0.619178\n",
      "episode 314 took 1028 iterations, avg loss = 0.020386, curr_eps = 0.618325\n",
      "episode 315 took 1104 iterations, avg loss = 0.018750, curr_eps = 0.617409\n",
      "episode 316 took 1056 iterations, avg loss = 0.017704, curr_eps = 0.616532\n",
      "episode 317 took 1332 iterations, avg loss = 0.021178, curr_eps = 0.615427\n",
      "episode 318 took 1252 iterations, avg loss = 0.018730, curr_eps = 0.614388\n",
      "episode 319 took 1188 iterations, avg loss = 0.017736, curr_eps = 0.613401\n",
      "episode 320 took 1340 iterations, avg loss = 0.015719, curr_eps = 0.612289\n",
      "episode 321 took 1400 iterations, avg loss = 0.017819, curr_eps = 0.611127\n",
      "episode 322 took 1024 iterations, avg loss = 0.016176, curr_eps = 0.610277\n",
      "episode 323 took 1016 iterations, avg loss = 0.019269, curr_eps = 0.609434\n",
      "episode 324 took 1264 iterations, avg loss = 0.019940, curr_eps = 0.608385\n",
      "episode 325 took 1308 iterations, avg loss = 0.020418, curr_eps = 0.607299\n",
      "episode 326 took 1020 iterations, avg loss = 0.018455, curr_eps = 0.606453\n",
      "episode 327 took 1236 iterations, avg loss = 0.018872, curr_eps = 0.605427\n",
      "episode 328 took 1292 iterations, avg loss = 0.020283, curr_eps = 0.604354\n",
      "episode 329 took 1056 iterations, avg loss = 0.017467, curr_eps = 0.603478\n",
      "episode 330 took 1112 iterations, avg loss = 0.018615, curr_eps = 0.602555\n",
      "episode 331 took 1144 iterations, avg loss = 0.016996, curr_eps = 0.601606\n",
      "episode 332 took 1104 iterations, avg loss = 0.018750, curr_eps = 0.600689\n",
      "episode 333 took 1220 iterations, avg loss = 0.019017, curr_eps = 0.599677\n",
      "episode 334 took 1216 iterations, avg loss = 0.019699, curr_eps = 0.598667\n",
      "episode 335 took 1088 iterations, avg loss = 0.019834, curr_eps = 0.597764\n",
      "episode 336 took 1312 iterations, avg loss = 0.021024, curr_eps = 0.596675\n",
      "episode 337 took 1004 iterations, avg loss = 0.022500, curr_eps = 0.595842\n",
      "episode 338 took 1008 iterations, avg loss = 0.021663, curr_eps = 0.595005\n",
      "episode 339 took 1196 iterations, avg loss = 0.019295, curr_eps = 0.594013\n",
      "episode 340 took 1348 iterations, avg loss = 0.017113, curr_eps = 0.592894\n",
      "episode 341 took 1088 iterations, avg loss = 0.015683, curr_eps = 0.591991\n",
      "episode 342 took 1368 iterations, avg loss = 0.018420, curr_eps = 0.590855\n",
      "episode 343 took 1104 iterations, avg loss = 0.017159, curr_eps = 0.589939\n",
      "episode 344 took 1064 iterations, avg loss = 0.019340, curr_eps = 0.589056\n",
      "episode 345 took 1056 iterations, avg loss = 0.017823, curr_eps = 0.588179\n",
      "episode 346 took 1348 iterations, avg loss = 0.019717, curr_eps = 0.587061\n",
      "episode 347 took 1088 iterations, avg loss = 0.015798, curr_eps = 0.586158\n",
      "episode 348 took 1128 iterations, avg loss = 0.016459, curr_eps = 0.585221\n",
      "episode 349 took 1028 iterations, avg loss = 0.018066, curr_eps = 0.584368\n",
      "episode 350 took 1172 iterations, avg loss = 0.019478, curr_eps = 0.583395\n",
      "episode 351 took 1104 iterations, avg loss = 0.017841, curr_eps = 0.582479\n",
      "episode 352 took 1124 iterations, avg loss = 0.020201, curr_eps = 0.581546\n",
      "episode 353 took 1112 iterations, avg loss = 0.017599, curr_eps = 0.580623\n",
      "episode 354 took 1268 iterations, avg loss = 0.015724, curr_eps = 0.579571\n",
      "episode 355 took 1204 iterations, avg loss = 0.017500, curr_eps = 0.578571\n",
      "episode 356 took 1528 iterations, avg loss = 0.017142, curr_eps = 0.577303\n",
      "episode 357 took 1008 iterations, avg loss = 0.016683, curr_eps = 0.576466\n",
      "episode 358 took 1100 iterations, avg loss = 0.018818, curr_eps = 0.575553\n",
      "episode 359 took 1224 iterations, avg loss = 0.016086, curr_eps = 0.574538\n",
      "episode 360 took 1316 iterations, avg loss = 0.019627, curr_eps = 0.573445\n",
      "episode 361 took 1512 iterations, avg loss = 0.018485, curr_eps = 0.572190\n",
      "episode 362 took 1104 iterations, avg loss = 0.019091, curr_eps = 0.571274\n",
      "episode 363 took 1344 iterations, avg loss = 0.014272, curr_eps = 0.570158\n",
      "episode 364 took 1024 iterations, avg loss = 0.017402, curr_eps = 0.569309\n",
      "episode 365 took 1024 iterations, avg loss = 0.016789, curr_eps = 0.568459\n",
      "episode 366 took 1328 iterations, avg loss = 0.018221, curr_eps = 0.567356\n",
      "episode 367 took 1180 iterations, avg loss = 0.020089, curr_eps = 0.566377\n",
      "episode 368 took 1156 iterations, avg loss = 0.017253, curr_eps = 0.565418\n",
      "episode 369 took 1308 iterations, avg loss = 0.018884, curr_eps = 0.564332\n",
      "episode 370 took 1020 iterations, avg loss = 0.017963, curr_eps = 0.563485\n",
      "episode 371 took 1220 iterations, avg loss = 0.018195, curr_eps = 0.562473\n",
      "episode 372 took 996 iterations, avg loss = 0.016759, curr_eps = 0.561646\n",
      "episode 373 took 1048 iterations, avg loss = 0.021432, curr_eps = 0.560776\n",
      "episode 374 took 1096 iterations, avg loss = 0.019460, curr_eps = 0.559866\n",
      "episode 375 took 1108 iterations, avg loss = 0.017663, curr_eps = 0.558947\n",
      "episode 376 took 1324 iterations, avg loss = 0.018182, curr_eps = 0.557848\n",
      "episode 377 took 1156 iterations, avg loss = 0.019314, curr_eps = 0.556888\n",
      "episode 378 took 1164 iterations, avg loss = 0.022629, curr_eps = 0.555922\n",
      "episode 379 took 1000 iterations, avg loss = 0.016064, curr_eps = 0.555092\n",
      "episode 380 took 1188 iterations, avg loss = 0.016047, curr_eps = 0.554106\n",
      "episode 381 took 1084 iterations, avg loss = 0.017824, curr_eps = 0.553207\n",
      "episode 382 took 1232 iterations, avg loss = 0.017610, curr_eps = 0.552184\n",
      "episode 383 took 1096 iterations, avg loss = 0.019345, curr_eps = 0.551274\n",
      "episode 384 took 1104 iterations, avg loss = 0.018636, curr_eps = 0.550358\n",
      "episode 385 took 1344 iterations, avg loss = 0.017257, curr_eps = 0.549242\n",
      "episode 386 took 1032 iterations, avg loss = 0.017996, curr_eps = 0.548386\n",
      "episode 387 took 1456 iterations, avg loss = 0.018337, curr_eps = 0.547177\n",
      "episode 388 took 1016 iterations, avg loss = 0.017910, curr_eps = 0.546334\n",
      "episode 389 took 1192 iterations, avg loss = 0.018203, curr_eps = 0.545345\n",
      "episode 390 took 1252 iterations, avg loss = 0.017829, curr_eps = 0.544306\n",
      "episode 391 took 1200 iterations, avg loss = 0.017977, curr_eps = 0.543310\n",
      "episode 392 took 1136 iterations, avg loss = 0.017116, curr_eps = 0.542367\n",
      "episode 393 took 1328 iterations, avg loss = 0.018221, curr_eps = 0.541265\n",
      "episode 394 took 1296 iterations, avg loss = 0.017608, curr_eps = 0.540189\n",
      "episode 395 took 1228 iterations, avg loss = 0.016748, curr_eps = 0.539170\n",
      "episode 396 took 1092 iterations, avg loss = 0.018153, curr_eps = 0.538263\n",
      "episode 397 took 1148 iterations, avg loss = 0.018575, curr_eps = 0.537310\n",
      "episode 398 took 1188 iterations, avg loss = 0.015836, curr_eps = 0.536324\n",
      "episode 399 took 1072 iterations, avg loss = 0.019312, curr_eps = 0.535435\n",
      "episode 400 took 1092 iterations, avg loss = 0.017463, curr_eps = 0.534528\n",
      "episode 401 took 1268 iterations, avg loss = 0.017702, curr_eps = 0.533476\n",
      "episode 402 took 1040 iterations, avg loss = 0.017736, curr_eps = 0.532613\n",
      "episode 403 took 1192 iterations, avg loss = 0.017992, curr_eps = 0.531623\n",
      "episode 404 took 1092 iterations, avg loss = 0.016544, curr_eps = 0.530717\n",
      "episode 405 took 1228 iterations, avg loss = 0.016544, curr_eps = 0.529698\n",
      "episode 406 took 1036 iterations, avg loss = 0.019380, curr_eps = 0.528838\n",
      "episode 407 took 1120 iterations, avg loss = 0.021505, curr_eps = 0.527908\n",
      "episode 408 took 1336 iterations, avg loss = 0.020083, curr_eps = 0.526799\n",
      "episode 409 took 1188 iterations, avg loss = 0.018581, curr_eps = 0.525813\n",
      "episode 410 took 1016 iterations, avg loss = 0.016304, curr_eps = 0.524970\n",
      "episode 411 took 1108 iterations, avg loss = 0.018456, curr_eps = 0.524050\n",
      "episode 412 took 1116 iterations, avg loss = 0.019447, curr_eps = 0.523124\n",
      "episode 413 took 1136 iterations, avg loss = 0.015680, curr_eps = 0.522181\n",
      "episode 414 took 1124 iterations, avg loss = 0.017188, curr_eps = 0.521248\n",
      "episode 415 took 1088 iterations, avg loss = 0.019488, curr_eps = 0.520345\n",
      "episode 416 took 1028 iterations, avg loss = 0.018188, curr_eps = 0.519492\n",
      "episode 417 took 1012 iterations, avg loss = 0.016617, curr_eps = 0.518652\n",
      "episode 418 took 1080 iterations, avg loss = 0.019633, curr_eps = 0.517756\n",
      "episode 419 took 1256 iterations, avg loss = 0.018870, curr_eps = 0.516713\n",
      "episode 420 took 1172 iterations, avg loss = 0.020227, curr_eps = 0.515740\n",
      "episode 421 took 1268 iterations, avg loss = 0.021064, curr_eps = 0.514688\n",
      "episode 422 took 1156 iterations, avg loss = 0.017578, curr_eps = 0.513728\n",
      "episode 423 took 1228 iterations, avg loss = 0.018587, curr_eps = 0.512709\n",
      "episode 424 took 1352 iterations, avg loss = 0.018546, curr_eps = 0.511587\n",
      "episode 425 took 1212 iterations, avg loss = 0.018212, curr_eps = 0.510581\n",
      "episode 426 took 1300 iterations, avg loss = 0.018229, curr_eps = 0.509502\n",
      "episode 427 took 1036 iterations, avg loss = 0.020228, curr_eps = 0.508642\n",
      "episode 428 took 1300 iterations, avg loss = 0.019290, curr_eps = 0.507563\n",
      "episode 429 took 1020 iterations, avg loss = 0.018578, curr_eps = 0.506717\n",
      "episode 430 took 1188 iterations, avg loss = 0.017948, curr_eps = 0.505731\n",
      "episode 431 took 1340 iterations, avg loss = 0.018806, curr_eps = 0.504618\n",
      "episode 432 took 1132 iterations, avg loss = 0.019060, curr_eps = 0.503679\n",
      "episode 433 took 1116 iterations, avg loss = 0.018323, curr_eps = 0.502753\n",
      "episode 434 took 1020 iterations, avg loss = 0.018947, curr_eps = 0.501906\n",
      "episode 435 took 1008 iterations, avg loss = 0.017057, curr_eps = 0.501069\n",
      "episode 436 took 1184 iterations, avg loss = 0.018114, curr_eps = 0.500087\n",
      "episode 437 took 1028 iterations, avg loss = 0.020142, curr_eps = 0.499233\n",
      "episode 438 took 1460 iterations, avg loss = 0.017771, curr_eps = 0.498022\n",
      "episode 439 took 1168 iterations, avg loss = 0.018363, curr_eps = 0.497052\n",
      "episode 440 took 1576 iterations, avg loss = 0.019641, curr_eps = 0.495744\n",
      "episode 441 took 1128 iterations, avg loss = 0.017460, curr_eps = 0.494808\n",
      "episode 442 took 1072 iterations, avg loss = 0.017556, curr_eps = 0.493918\n",
      "episode 443 took 1104 iterations, avg loss = 0.017614, curr_eps = 0.493002\n",
      "episode 444 took 1116 iterations, avg loss = 0.017761, curr_eps = 0.492075\n",
      "episode 445 took 1184 iterations, avg loss = 0.018220, curr_eps = 0.491093\n",
      "episode 446 took 1188 iterations, avg loss = 0.017631, curr_eps = 0.490107\n",
      "episode 447 took 1048 iterations, avg loss = 0.018678, curr_eps = 0.489237\n",
      "episode 448 took 1140 iterations, avg loss = 0.019696, curr_eps = 0.488291\n",
      "episode 449 took 1112 iterations, avg loss = 0.019968, curr_eps = 0.487368\n",
      "episode 450 took 1212 iterations, avg loss = 0.016970, curr_eps = 0.486362\n",
      "episode 451 took 1228 iterations, avg loss = 0.016646, curr_eps = 0.485342\n",
      "episode 452 took 1072 iterations, avg loss = 0.018961, curr_eps = 0.484453\n",
      "episode 453 took 1396 iterations, avg loss = 0.019935, curr_eps = 0.483294\n",
      "episode 454 took 1208 iterations, avg loss = 0.017234, curr_eps = 0.482291\n",
      "episode 455 took 1388 iterations, avg loss = 0.018876, curr_eps = 0.481139\n",
      "episode 456 took 1028 iterations, avg loss = 0.020020, curr_eps = 0.480286\n",
      "episode 457 took 1016 iterations, avg loss = 0.018157, curr_eps = 0.479443\n",
      "episode 458 took 1100 iterations, avg loss = 0.019275, curr_eps = 0.478530\n",
      "episode 459 took 1160 iterations, avg loss = 0.018599, curr_eps = 0.477567\n",
      "episode 460 took 1076 iterations, avg loss = 0.019123, curr_eps = 0.476674\n",
      "episode 461 took 1040 iterations, avg loss = 0.018581, curr_eps = 0.475811\n",
      "episode 462 took 1200 iterations, avg loss = 0.016722, curr_eps = 0.474815\n",
      "episode 463 took 1136 iterations, avg loss = 0.017116, curr_eps = 0.473872\n",
      "episode 464 took 1120 iterations, avg loss = 0.019041, curr_eps = 0.472942\n",
      "episode 465 took 1456 iterations, avg loss = 0.019370, curr_eps = 0.471734\n",
      "episode 466 took 1108 iterations, avg loss = 0.019248, curr_eps = 0.470814\n",
      "episode 467 took 1128 iterations, avg loss = 0.019573, curr_eps = 0.469878\n",
      "episode 468 took 1288 iterations, avg loss = 0.021223, curr_eps = 0.468809\n",
      "episode 469 took 1728 iterations, avg loss = 0.020882, curr_eps = 0.467375\n",
      "episode 470 took 1372 iterations, avg loss = 0.021199, curr_eps = 0.466236\n",
      "episode 471 took 1032 iterations, avg loss = 0.019334, curr_eps = 0.465379\n",
      "episode 472 took 1100 iterations, avg loss = 0.018134, curr_eps = 0.464466\n",
      "episode 473 took 1024 iterations, avg loss = 0.019608, curr_eps = 0.463616\n"
     ]
    }
   ],
   "source": [
    "qq.train(max_t=100000, max_ep=100000, \n",
    "         max_frames=10000000, update_q=True, debug=True, eps=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-20 07:52:22,600] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "qq = DeepQ(env)\n",
    "qq.load_weights_from(\"weights.pkl.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbeckham/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:118: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 took 1112 iterations, avg loss = 0.016778, curr_eps = 0.459601\n",
      "episode 2 took 1352 iterations, avg loss = 0.015671, curr_eps = 0.459114\n",
      "episode 3 took 1556 iterations, avg loss = 0.011034, curr_eps = 0.458554\n",
      "episode 4 took 1164 iterations, avg loss = 0.014009, curr_eps = 0.458135\n",
      "episode 5 took 1104 iterations, avg loss = 0.013409, curr_eps = 0.457738\n",
      "episode 6 took 1020 iterations, avg loss = 0.011442, curr_eps = 0.457371\n",
      "episode 7 took 1024 iterations, avg loss = 0.014951, curr_eps = 0.457002\n",
      "episode 8 took 1220 iterations, avg loss = 0.016242, curr_eps = 0.456563\n",
      "episode 9 took 1180 iterations, avg loss = 0.014137, curr_eps = 0.456138\n",
      "episode 10 took 1120 iterations, avg loss = 0.017249, curr_eps = 0.455735\n",
      "episode 11 took 1224 iterations, avg loss = 0.018135, curr_eps = 0.455294\n",
      "episode 12 took 1104 iterations, avg loss = 0.015455, curr_eps = 0.454897\n",
      "episode 13 took 1172 iterations, avg loss = 0.016802, curr_eps = 0.454475\n",
      "episode 14 took 1084 iterations, avg loss = 0.019560, curr_eps = 0.454084\n",
      "episode 15 took 1016 iterations, avg loss = 0.018281, curr_eps = 0.453719\n",
      "episode 16 took 1488 iterations, avg loss = 0.015836, curr_eps = 0.453183\n",
      "episode 17 took 1324 iterations, avg loss = 0.018466, curr_eps = 0.452706\n",
      "episode 18 took 1192 iterations, avg loss = 0.017466, curr_eps = 0.452277\n",
      "episode 19 took 1048 iterations, avg loss = 0.018319, curr_eps = 0.451900\n",
      "episode 20 took 1224 iterations, avg loss = 0.017520, curr_eps = 0.451459\n",
      "episode 21 took 1192 iterations, avg loss = 0.016414, curr_eps = 0.451030\n",
      "episode 22 took 1104 iterations, avg loss = 0.021250, curr_eps = 0.450633\n",
      "episode 23 took 1100 iterations, avg loss = 0.018134, curr_eps = 0.450237\n",
      "episode 24 took 1120 iterations, avg loss = 0.017137, curr_eps = 0.449834\n",
      "episode 25 took 1260 iterations, avg loss = 0.020402, curr_eps = 0.449380\n",
      "episode 26 took 1344 iterations, avg loss = 0.018377, curr_eps = 0.448896\n",
      "episode 27 took 1096 iterations, avg loss = 0.019918, curr_eps = 0.448502\n",
      "episode 28 took 1036 iterations, avg loss = 0.017442, curr_eps = 0.448129\n",
      "episode 29 took 1084 iterations, avg loss = 0.016782, curr_eps = 0.447738\n",
      "episode 30 took 1132 iterations, avg loss = 0.019282, curr_eps = 0.447331\n",
      "episode 31 took 1060 iterations, avg loss = 0.020123, curr_eps = 0.446949\n",
      "episode 32 took 1028 iterations, avg loss = 0.019775, curr_eps = 0.446579\n",
      "episode 33 took 1240 iterations, avg loss = 0.015271, curr_eps = 0.446133\n",
      "episode 34 took 1024 iterations, avg loss = 0.019975, curr_eps = 0.445764\n",
      "episode 35 took 1024 iterations, avg loss = 0.017279, curr_eps = 0.445396\n",
      "episode 36 took 1100 iterations, avg loss = 0.019959, curr_eps = 0.445000\n",
      "episode 37 took 1024 iterations, avg loss = 0.016912, curr_eps = 0.444631\n",
      "episode 38 took 1200 iterations, avg loss = 0.018917, curr_eps = 0.444199\n",
      "episode 39 took 1024 iterations, avg loss = 0.016667, curr_eps = 0.443830\n",
      "episode 40 took 1124 iterations, avg loss = 0.018415, curr_eps = 0.443426\n",
      "episode 41 took 1200 iterations, avg loss = 0.018708, curr_eps = 0.442994\n",
      "episode 42 took 1492 iterations, avg loss = 0.019153, curr_eps = 0.442456\n",
      "episode 43 took 1020 iterations, avg loss = 0.017101, curr_eps = 0.442089\n",
      "episode 44 took 1264 iterations, avg loss = 0.019048, curr_eps = 0.441634\n",
      "episode 45 took 1056 iterations, avg loss = 0.019011, curr_eps = 0.441254\n",
      "episode 46 took 1120 iterations, avg loss = 0.016801, curr_eps = 0.440851\n",
      "episode 47 took 1096 iterations, avg loss = 0.018658, curr_eps = 0.440456\n",
      "episode 48 took 1208 iterations, avg loss = 0.016715, curr_eps = 0.440021\n",
      "episode 49 took 1172 iterations, avg loss = 0.020334, curr_eps = 0.439600\n",
      "episode 50 took 1288 iterations, avg loss = 0.018400, curr_eps = 0.439136\n",
      "episode 51 took 1252 iterations, avg loss = 0.016727, curr_eps = 0.438685\n",
      "episode 52 took 1008 iterations, avg loss = 0.021414, curr_eps = 0.438322\n",
      "episode 53 took 1092 iterations, avg loss = 0.020335, curr_eps = 0.437929\n",
      "episode 54 took 1028 iterations, avg loss = 0.018433, curr_eps = 0.437559\n",
      "episode 55 took 1364 iterations, avg loss = 0.016544, curr_eps = 0.437068\n",
      "episode 56 took 1384 iterations, avg loss = 0.018297, curr_eps = 0.436570\n",
      "episode 57 took 1184 iterations, avg loss = 0.019597, curr_eps = 0.436144\n",
      "episode 58 took 1132 iterations, avg loss = 0.021277, curr_eps = 0.435736\n",
      "episode 59 took 1200 iterations, avg loss = 0.022262, curr_eps = 0.435304\n",
      "episode 60 took 1108 iterations, avg loss = 0.020267, curr_eps = 0.434905\n",
      "episode 61 took 1308 iterations, avg loss = 0.018597, curr_eps = 0.434434\n",
      "episode 62 took 1012 iterations, avg loss = 0.018229, curr_eps = 0.434070\n",
      "episode 63 took 1088 iterations, avg loss = 0.018681, curr_eps = 0.433678\n",
      "episode 64 took 1176 iterations, avg loss = 0.020798, curr_eps = 0.433255\n",
      "episode 65 took 1132 iterations, avg loss = 0.015625, curr_eps = 0.432847\n",
      "episode 66 took 1168 iterations, avg loss = 0.016538, curr_eps = 0.432427\n",
      "episode 67 took 1020 iterations, avg loss = 0.016609, curr_eps = 0.432060\n",
      "episode 68 took 1028 iterations, avg loss = 0.017090, curr_eps = 0.431690\n",
      "episode 69 took 1188 iterations, avg loss = 0.019426, curr_eps = 0.431262\n",
      "episode 70 took 1124 iterations, avg loss = 0.018750, curr_eps = 0.430857\n",
      "episode 71 took 1012 iterations, avg loss = 0.016493, curr_eps = 0.430493\n",
      "episode 72 took 1004 iterations, avg loss = 0.019375, curr_eps = 0.430132\n",
      "episode 73 took 1012 iterations, avg loss = 0.022073, curr_eps = 0.429767\n",
      "episode 74 took 1092 iterations, avg loss = 0.018267, curr_eps = 0.429374\n",
      "episode 75 took 1320 iterations, avg loss = 0.018522, curr_eps = 0.428899\n",
      "episode 76 took 1228 iterations, avg loss = 0.019097, curr_eps = 0.428457\n",
      "episode 77 took 1040 iterations, avg loss = 0.020512, curr_eps = 0.428082\n",
      "episode 78 took 1028 iterations, avg loss = 0.016235, curr_eps = 0.427712\n",
      "episode 79 took 1124 iterations, avg loss = 0.017746, curr_eps = 0.427308\n",
      "episode 80 took 1600 iterations, avg loss = 0.016447, curr_eps = 0.426732\n",
      "episode 81 took 1100 iterations, avg loss = 0.018362, curr_eps = 0.426336\n",
      "episode 82 took 1192 iterations, avg loss = 0.019886, curr_eps = 0.425907\n",
      "episode 83 took 1224 iterations, avg loss = 0.020389, curr_eps = 0.425466\n",
      "episode 84 took 1100 iterations, avg loss = 0.021214, curr_eps = 0.425070\n",
      "episode 85 took 1112 iterations, avg loss = 0.017712, curr_eps = 0.424670\n",
      "episode 86 took 1016 iterations, avg loss = 0.019145, curr_eps = 0.424304\n",
      "episode 87 took 1108 iterations, avg loss = 0.018456, curr_eps = 0.423905\n",
      "episode 88 took 1100 iterations, avg loss = 0.020871, curr_eps = 0.423509\n",
      "episode 89 took 1168 iterations, avg loss = 0.019867, curr_eps = 0.423088\n",
      "episode 90 took 1356 iterations, avg loss = 0.017289, curr_eps = 0.422600\n",
      "episode 91 took 1016 iterations, avg loss = 0.018651, curr_eps = 0.422235\n",
      "episode 92 took 1024 iterations, avg loss = 0.015441, curr_eps = 0.421866\n",
      "episode 93 took 1036 iterations, avg loss = 0.020954, curr_eps = 0.421493\n",
      "episode 94 took 1004 iterations, avg loss = 0.018125, curr_eps = 0.421132\n",
      "episode 95 took 1020 iterations, avg loss = 0.019685, curr_eps = 0.420764\n",
      "episode 96 took 1132 iterations, avg loss = 0.018174, curr_eps = 0.420357\n",
      "episode 97 took 1060 iterations, avg loss = 0.018111, curr_eps = 0.419975\n",
      "episode 98 took 1040 iterations, avg loss = 0.021356, curr_eps = 0.419601\n",
      "episode 99 took 1016 iterations, avg loss = 0.021616, curr_eps = 0.419235\n",
      "episode 100 took 1140 iterations, avg loss = 0.019916, curr_eps = 0.418825\n",
      "episode 101 took 1140 iterations, avg loss = 0.021127, curr_eps = 0.418414\n",
      "episode 102 took 1140 iterations, avg loss = 0.015735, curr_eps = 0.418004\n",
      "episode 103 took 1184 iterations, avg loss = 0.019280, curr_eps = 0.417578\n",
      "episode 104 took 1012 iterations, avg loss = 0.018973, curr_eps = 0.417213\n",
      "episode 105 took 1120 iterations, avg loss = 0.017697, curr_eps = 0.416810\n",
      "episode 106 took 1148 iterations, avg loss = 0.019996, curr_eps = 0.416397\n",
      "episode 107 took 1252 iterations, avg loss = 0.017728, curr_eps = 0.415946\n",
      "episode 108 took 1160 iterations, avg loss = 0.019247, curr_eps = 0.415528\n",
      "episode 109 took 1252 iterations, avg loss = 0.015825, curr_eps = 0.415078\n",
      "episode 110 took 1056 iterations, avg loss = 0.016754, curr_eps = 0.414698\n",
      "episode 111 took 1384 iterations, avg loss = 0.016214, curr_eps = 0.414199\n",
      "episode 112 took 1012 iterations, avg loss = 0.018353, curr_eps = 0.413835\n",
      "episode 113 took 1112 iterations, avg loss = 0.018051, curr_eps = 0.413435\n",
      "episode 114 took 1328 iterations, avg loss = 0.016711, curr_eps = 0.412957\n",
      "episode 115 took 1124 iterations, avg loss = 0.017746, curr_eps = 0.412552\n",
      "episode 116 took 1016 iterations, avg loss = 0.019516, curr_eps = 0.412186\n",
      "episode 117 took 1184 iterations, avg loss = 0.016208, curr_eps = 0.411760\n",
      "episode 118 took 1020 iterations, avg loss = 0.019562, curr_eps = 0.411393\n",
      "episode 119 took 1116 iterations, avg loss = 0.016974, curr_eps = 0.410991\n",
      "episode 120 took 1248 iterations, avg loss = 0.019895, curr_eps = 0.410542\n",
      "episode 121 took 1104 iterations, avg loss = 0.021818, curr_eps = 0.410144\n",
      "episode 122 took 1084 iterations, avg loss = 0.015856, curr_eps = 0.409754\n",
      "episode 123 took 1028 iterations, avg loss = 0.019409, curr_eps = 0.409384\n",
      "episode 124 took 1360 iterations, avg loss = 0.019082, curr_eps = 0.408894\n",
      "episode 125 took 1132 iterations, avg loss = 0.018063, curr_eps = 0.408487\n",
      "episode 126 took 1144 iterations, avg loss = 0.018202, curr_eps = 0.408075\n",
      "episode 127 took 1380 iterations, avg loss = 0.018623, curr_eps = 0.407578\n",
      "episode 128 took 1184 iterations, avg loss = 0.018326, curr_eps = 0.407152\n",
      "episode 129 took 1044 iterations, avg loss = 0.020072, curr_eps = 0.406776\n",
      "episode 130 took 1224 iterations, avg loss = 0.019980, curr_eps = 0.406336\n",
      "episode 131 took 1020 iterations, avg loss = 0.017717, curr_eps = 0.405968\n",
      "episode 132 took 1020 iterations, avg loss = 0.020054, curr_eps = 0.405601\n",
      "episode 133 took 1092 iterations, avg loss = 0.019072, curr_eps = 0.405208\n",
      "episode 134 took 1184 iterations, avg loss = 0.019174, curr_eps = 0.404782\n",
      "episode 135 took 1252 iterations, avg loss = 0.021334, curr_eps = 0.404331\n",
      "episode 136 took 1144 iterations, avg loss = 0.016996, curr_eps = 0.403919\n",
      "episode 137 took 1100 iterations, avg loss = 0.020187, curr_eps = 0.403523\n",
      "episode 138 took 1168 iterations, avg loss = 0.022122, curr_eps = 0.403103\n",
      "episode 139 took 1180 iterations, avg loss = 0.018282, curr_eps = 0.402678\n",
      "episode 140 took 1480 iterations, avg loss = 0.018377, curr_eps = 0.402145\n",
      "episode 141 took 1060 iterations, avg loss = 0.020597, curr_eps = 0.401764\n",
      "episode 142 took 1088 iterations, avg loss = 0.017989, curr_eps = 0.401372\n",
      "episode 143 took 1192 iterations, avg loss = 0.019045, curr_eps = 0.400943\n",
      "episode 144 took 1012 iterations, avg loss = 0.020957, curr_eps = 0.400578\n",
      "episode 145 took 1024 iterations, avg loss = 0.020466, curr_eps = 0.400210\n",
      "episode 146 took 1312 iterations, avg loss = 0.020833, curr_eps = 0.399737\n",
      "episode 147 took 1112 iterations, avg loss = 0.019404, curr_eps = 0.399337\n",
      "episode 148 took 1020 iterations, avg loss = 0.017347, curr_eps = 0.398970\n",
      "episode 149 took 1056 iterations, avg loss = 0.018061, curr_eps = 0.398590\n",
      "episode 150 took 1252 iterations, avg loss = 0.017528, curr_eps = 0.398139\n",
      "episode 151 took 1344 iterations, avg loss = 0.017631, curr_eps = 0.397655\n",
      "episode 152 took 1308 iterations, avg loss = 0.020418, curr_eps = 0.397184\n",
      "episode 153 took 1104 iterations, avg loss = 0.018523, curr_eps = 0.396787\n",
      "episode 154 took 1196 iterations, avg loss = 0.016254, curr_eps = 0.396356\n",
      "episode 155 took 1120 iterations, avg loss = 0.019153, curr_eps = 0.395953\n",
      "episode 156 took 1088 iterations, avg loss = 0.019027, curr_eps = 0.395561\n",
      "episode 157 took 1184 iterations, avg loss = 0.019174, curr_eps = 0.395135\n",
      "episode 158 took 1208 iterations, avg loss = 0.019726, curr_eps = 0.394700\n",
      "episode 159 took 1168 iterations, avg loss = 0.018256, curr_eps = 0.394280\n",
      "episode 160 took 1028 iterations, avg loss = 0.018555, curr_eps = 0.393910\n",
      "episode 161 took 1064 iterations, avg loss = 0.019811, curr_eps = 0.393527\n",
      "episode 162 took 1180 iterations, avg loss = 0.018070, curr_eps = 0.393102\n",
      "episode 163 took 1100 iterations, avg loss = 0.017450, curr_eps = 0.392706\n",
      "episode 164 took 1120 iterations, avg loss = 0.018369, curr_eps = 0.392303\n",
      "episode 165 took 1024 iterations, avg loss = 0.020833, curr_eps = 0.391934\n",
      "episode 166 took 1032 iterations, avg loss = 0.018847, curr_eps = 0.391563\n",
      "episode 167 took 1044 iterations, avg loss = 0.017668, curr_eps = 0.391187\n",
      "episode 168 took 1024 iterations, avg loss = 0.019485, curr_eps = 0.390818\n",
      "episode 169 took 1100 iterations, avg loss = 0.019389, curr_eps = 0.390422\n",
      "episode 170 took 1012 iterations, avg loss = 0.017733, curr_eps = 0.390058\n",
      "episode 171 took 1096 iterations, avg loss = 0.017628, curr_eps = 0.389663\n",
      "episode 172 took 1364 iterations, avg loss = 0.017555, curr_eps = 0.389172\n",
      "episode 173 took 1184 iterations, avg loss = 0.021081, curr_eps = 0.388746\n",
      "episode 174 took 1012 iterations, avg loss = 0.019097, curr_eps = 0.388382\n",
      "episode 175 took 1384 iterations, avg loss = 0.018207, curr_eps = 0.387883\n",
      "episode 176 took 1132 iterations, avg loss = 0.016733, curr_eps = 0.387476\n",
      "episode 177 took 1128 iterations, avg loss = 0.017349, curr_eps = 0.387070\n",
      "episode 178 took 1140 iterations, avg loss = 0.018266, curr_eps = 0.386659\n",
      "episode 179 took 1320 iterations, avg loss = 0.018617, curr_eps = 0.386184\n",
      "episode 180 took 1352 iterations, avg loss = 0.017248, curr_eps = 0.385697\n",
      "episode 181 took 1016 iterations, avg loss = 0.019516, curr_eps = 0.385332\n",
      "episode 182 took 1196 iterations, avg loss = 0.018981, curr_eps = 0.384901\n",
      "episode 183 took 1020 iterations, avg loss = 0.019685, curr_eps = 0.384534\n",
      "episode 184 took 1020 iterations, avg loss = 0.019070, curr_eps = 0.384167\n"
     ]
    }
   ],
   "source": [
    "qq.train(max_t=100000, max_ep=100000, \n",
    "         max_frames=1000000, update_q=True, debug=True, eps=0.46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notes:\n",
    "* On my Macbook, one episode (1150 frames) took 12 sec. This means to preocess 10M frames, it will take ~ 28 hrs. ??\n",
    "* https://jkjung-avt.github.io/dqn-pong/ says it took 'less than a day' on a 1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "88\n",
      "187\n",
      "254\n",
      "277\n",
      "374\n",
      "398\n",
      "432\n",
      "501\n",
      "643\n",
      "648\n",
      "728\n",
      "821\n",
      "904\n",
      "928\n",
      "987\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(tmp1)):\n",
    "    if tmp1[idx][\"r_t\"] == -1:\n",
    "        print idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAADfCAYAAAANzu44AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNEuSJEkZDs2SJElSxv8HEoJvrTTwDbQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37918eb9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs0AAADfCAYAAAANzu44AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVKGQ7MkSZKU8f8Bn6Vvq2a38DcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3795f7ce90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx=904\n",
    "subplot_phi(tmp1[idx][\"phi_t\"])\n",
    "print tmp1[idx][\"r_t\"]\n",
    "subplot_phi(tmp1[idx][\"phi_t1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def subplot_phi(phi):\n",
    "    \"\"\"\n",
    "    :phi: of shape (4, H, W)\n",
    "    \"\"\"\n",
    "    assert phi.shape[0] == 4\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(phi[0],cmap=\"gray\")\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(phi[1],cmap=\"gray\")\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(phi[2],cmap=\"gray\")\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(phi[3],cmap=\"gray\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
